# distil_model
[Distillation](https://arxiv.org/pdf/1503.02531.pdf) is to transfer knowledge from a large network (teacher) to a smaller network (student).

## Suggested papers to read
* [Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf) - [implementation](./distil_nn)
* [Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks](https://arxiv.org/pdf/2004.05937.pdf)
* [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)
* [FitNets: Hints for Thin Deep Nets](https://arxiv.org/pdf/1412.6550.pdf)

@article{
	ngo_2021,
	journal={A Review of Model Distillation},
	author={Ngo, Dat Q},
	year={2021}
} 
