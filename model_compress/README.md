# model_compress
Model compression is a hot research topic when DNNs become larger and larger. The results are that the inference time could be 3-4 seconds for a GEC model to correct grammar mistakes in a sentence.

## Quantization

## distil_model
[Distillation](https://arxiv.org/pdf/1503.02531.pdf) is to transfer knowledge from a large network (teacher) to a smaller network (student).

### Suggested papers to read
* [Distilling the Knowledge in a Neural Network](https://arxiv.org/pdf/1503.02531.pdf) - [implementation](./distil_nn)
* [Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks](https://arxiv.org/pdf/2004.05937.pdf)
* [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)
* [FitNets: Hints for Thin Deep Nets](https://arxiv.org/pdf/1412.6550.pdf)

@article{
        ngo_2021,
        journal={A Review of Model Distillation},
        author={Ngo, Dat Q},
        year={2021}
}
